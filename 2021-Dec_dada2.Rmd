---
title: "2021-Dec"
output: html_document
---

#Load Libraries
```{r}
library(readr)
library(fs)
library(dplyr)
library(tibble)
library(Biostrings)
library(dada2)
library(stringr)
library(magrittr)
library(ggplot2)
library(phyloseq)
library(tidyr)
```

#Setup
```{r}
#Directories
data.dir = "/work/kdm65"
demux.dir = "/work/kdm65/16S_Dec_2021"
demux.redo.dir = "/work/kdm65/16S_Mar_2022"
output.dir = "/work/kdm65/scratch"
```

```{r}
if (dir_exists(output.dir)) {
  dir_delete(output.dir)
}
dir_create(output.dir)
```

```{r}
#Files
silva.ref = file.path(data.dir, "silva_nr99_v138.1_wSpecies_train_set.fa.gz")

#Initial Run
map.file = file.path(data.dir, "16S_metadata.txt")
ps.rds = file.path(data.dir, "2021-Dec.rds")

#Redone Run
redo.map.file = file.path(data.dir, "16S_metadata_redo.txt")
redo.ps.rds = file.path(data.dir, "2021-Dec_redo.rds")

#Merged Runs
merged.ps.rds = file.path(data.dir, "2021-Dec_merged.rds")
```


# Initial Run
## Filter and Trim
Get lists of forward and reverse reads
```{r}
fnFs <- sort(list.files(demux.dir, pattern = "_L001_R1_001.fastq.gz", full.names = TRUE))
fnRs <- sort(list.files(demux.dir, pattern = "_L001_R2_001.fastq.gz", full.names = TRUE))

forward_fastq_suffix = "_L001_R1_001.fastq.gz"

fnFs %>%
  basename %>%
  str_replace(forward_fastq_suffix,"") ->
  sample.names
```

```{r}
print(fnFs)
```

```{r}
print(fnRs)
```

```{r}
print(sample.names)
```

## Quality Profiles
```{r}
plotQualityProfile(fnFs[1:2])
```

```{r}
plotQualityProfile(fnRs[1:2])
```

## Filter Reads
Assign filepaths for filtered files
```{r}
filt_path <- file.path(output.dir, "filtered")
filtFs <- file.path(filt_path, paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(filt_path, paste0(sample.names, "_R_filt.fastq.gz"))
```

Filter reads
```{r}
filt.out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, trimLeft = 10, truncLen = c(245,245),
                          maxN = 0, maxEE = c(2,2), truncQ = 2, rm.phix = TRUE,
                          compress = TRUE, multithread = TRUE)
```

```{r}
head(filt.out)
```

## Learn Error Rates
```{r}
errF <- learnErrors(filtFs, multithread = TRUE)
errR <- learnErrors(filtRs, multithread = TRUE)
```

```{r}
plotErrors(errF, nominalQ = TRUE)
```

## Dereplication
```{r}
derepFs <- derepFastq(filtFs, verbose = TRUE)
derepRs <- derepFastq(filtRs, verbose = TRUE)
names(derepFs) <- sample.names
names(derepRs) <- sample.names
```

## Sample Inference
```{r}
dadaFs <- dada(derepFs, err=errF, multithread = TRUE)
dadaRs <- dada(derepRs, err=errR, multithread = TRUE)
```

```{r}
dadaFs[[2]]
```

## Merge Paired Reads
```{r}
mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose = TRUE)
```

```{r}
head(mergers[[2]])
```

## Further Processing
Construct sequence table
```{r}
seqtab <- makeSequenceTable(mergers)
dim(seqtab)
```

```{r}
table(nchar(getSequences(seqtab)))
```

```{r}
seqtab2 <- seqtab[, nchar(colnames(seqtab)) %in% seq(417,449)]
```

Remove chimeras
```{r}
seqtab.nochim <- removeBimeraDenovo(seqtab2, method = "consensus", multithread = TRUE, verbose = TRUE)
dim(seqtab.nochim)
```

```{r}
sum(seqtab.nochim)/sum(seqtab2)
```

Track reads through the pipeline
```{r}
getN <- function(x) sum(getUniques(x))
filt.out %>%
  as_tibble(rownames = "filename") %>%
  mutate(sample = str_replace(filename, forward_fastq_suffix, "")) %>%
  select(sample, input = reads.in, filtered = reads.out) ->
  track

sapply(dadaFs, getN) %>%
  enframe(name = "sample", value = "denoised") ->
  denoised
track %<>% full_join(denoised, by = c("sample"))

sapply(mergers, getN) %>%
  enframe(name = "sample", value = "merged") ->
  merged
track %<>% full_join(merged, by = c("sample"))

rowSums(seqtab2) %>%
  enframe(name = "sample", value = "tabled") ->
  tabled
track %<>% full_join(tabled, by = c("sample"))

rowSums(seqtab.nochim) %>%
  enframe(name = "sample", value = "nonchim") ->
  nonchim
track %<>% full_join(nonchim, by = c("sample"))

track
```

```{r}
track %>%
  gather(key = "stage", value = "counts", -c("sample")) %>%
  replace_na(list(counts = 0)) %>%
  mutate(stage = factor(stage, levels = c('input', 'filtered', 'denoised', 'merged', 'tabled', 'nonchim'))) %>%
  ggplot(mapping = aes(x = stage, y = counts, by = sample, group = sample)) + geom_line(alpha = 0.05) + theme_classic()
```

## Assign Taxonomy
```{r}
taxa <- assignTaxonomy(seqtab.nochim, silva.ref, multithread = TRUE)
taxa.print <- taxa
rownames(taxa.print) <- NULL
head(taxa.print)
```

## Make Phyloseq Object
Load metadata
```{r}
metadata.df = read_tsv(map.file)

sample.ids = metadata.df$SampleID
good.ids = paste0("7455-L", sample.ids, "_S", sample.ids)

metadata.df$SampleID = good.ids
metadata.df = metadata.df %>%
  column_to_rownames("SampleID") %>%
  as.data.frame()

#metadata.df = read_tsv(map.file) %>%
#  column_to_rownames("SampleID") %>%
#  as.data.frame()

metadata.df
```
```{r}
metadata.df = metadata.df[1:92,]

metadata.df
```

Construct phyloseq object
```{r}
otus = otu_table(seqtab.nochim, taxa_are_rows = FALSE)
sd = sample_data(metadata.df)
ps <- phyloseq(otus, sd, tax_table(taxa))

ps
```

Save phyloseq object as RDS
```{r}
write_rds(ps, ps.rds)
```

Confirm that the RDS is usable
```{r}
loaded.ps = read_rds(ps.rds)
print(loaded.ps)
```


# Redone Samples
## Filter and Trim
Get lists of forward and reverse reads
```{r}
fnFs.redo <- sort(list.files(demux.redo.dir, pattern = "_L001_R1_001.fastq.gz", full.names = TRUE))
fnRs.redo <- sort(list.files(demux.redo.dir, pattern = "_L001_R2_001.fastq.gz", full.names = TRUE))

forward_fastq_suffix.redo = "_L001_R1_001.fastq.gz"

fnFs.redo %>%
  basename %>%
  str_replace(forward_fastq_suffix.redo,"") ->
  sample.names.redo
```

```{r}
print(fnFs.redo)
```

```{r}
print(fnRs.redo)
```

```{r}
print(sample.names.redo)
```

## Quality Profiles
```{r}
plotQualityProfile(fnFs.redo[1:4])
```

```{r}
plotQualityProfile(fnRs.redo[1:4])
```
Samples 16, 25, and 51 didn't have a ton of reads. We're not sure what the problem was during the sequencing process for these, but I'm not too suprised that sample 16 and 25 here also tends to be of lower quality.


## Filter Reads
Assign filepaths for filtered files
```{r}
filt_path.redo <- file.path(output.dir, "filtered.redo")
filtFs.redo <- file.path(filt_path.redo, paste0(sample.names.redo, "_F_filt.fastq.gz"))
filtRs.redo <- file.path(filt_path.redo, paste0(sample.names.redo, "_R_filt.fastq.gz"))
```

Filter reads
```{r}
filt.out.redo <- filterAndTrim(fnFs.redo, filtFs.redo, fnRs.redo, filtRs.redo, trimLeft = 10, truncLen = c(245,245),
                          maxN = 0, maxEE = c(2,5), truncQ = 2, rm.phix = TRUE,
                          compress = TRUE, multithread = TRUE)
```

```{r}
head(filt.out.redo)
```

## Learn Error Rates
```{r}
errF.redo <- learnErrors(filtFs.redo, multithread = TRUE)
errR.redo <- learnErrors(filtRs.redo, multithread = TRUE)
```

```{r}
plotErrors(errF.redo, nominalQ = TRUE)
```

## Dereplication
```{r}
derepFs.redo <- derepFastq(filtFs.redo, verbose = TRUE)
derepRs.redo <- derepFastq(filtRs.redo, verbose = TRUE)
names(derepFs.redo) <- sample.names.redo
names(derepRs.redo) <- sample.names.redo
```

## Sample Inference
```{r}
dadaFs.redo <- dada(derepFs.redo, err=errF.redo, multithread = TRUE)
dadaRs.redo <- dada(derepRs.redo, err=errR.redo, multithread = TRUE)
```

```{r}
dadaFs.redo[[1]]
```

## Merge Paired Reads
```{r}
mergers.redo <- mergePairs(dadaFs.redo, derepFs.redo, dadaRs.redo, derepRs.redo, verbose = TRUE)
```

```{r}
head(mergers.redo[[1]])
```

## Further Processing
Construct sequence table
```{r}
seqtab.redo <- makeSequenceTable(mergers.redo)
dim(seqtab.redo)
```

```{r}
table(nchar(getSequences(seqtab.redo)))
```

```{r}
seqtab2.redo <- seqtab.redo[, nchar(colnames(seqtab.redo)) %in% seq(417,449)]
```

Remove chimeras
```{r}
seqtab.nochim.redo <- removeBimeraDenovo(seqtab2.redo, method = "consensus", multithread = TRUE, verbose = TRUE)
dim(seqtab.nochim.redo)
```

```{r}
sum(seqtab.nochim.redo)/sum(seqtab2.redo)
```

Track reads through the pipeline
```{r}
getN <- function(x) sum(getUniques(x))
filt.out.redo %>%
  as_tibble(rownames = "filename") %>%
  mutate(sample = str_replace(filename, forward_fastq_suffix.redo, "")) %>%
  select(sample, input = reads.in, filtered = reads.out) ->
  track.redo

sapply(dadaFs.redo, getN) %>%
  enframe(name = "sample", value = "denoised") ->
  denoised.redo
track.redo %<>% full_join(denoised.redo, by = c("sample"))

sapply(mergers.redo, getN) %>%
  enframe(name = "sample", value = "merged") ->
  merged.redo
track.redo %<>% full_join(merged.redo, by = c("sample"))

rowSums(seqtab2.redo) %>%
  enframe(name = "sample", value = "tabled") ->
  tabled.redo
track.redo %<>% full_join(tabled.redo, by = c("sample"))

rowSums(seqtab.nochim.redo) %>%
  enframe(name = "sample", value = "nonchim") ->
  nonchim.redo
track.redo %<>% full_join(nonchim.redo, by = c("sample"))

track.redo
```

```{r}
track.redo %>%
  gather(key = "stage", value = "counts", -c("sample")) %>%
  replace_na(list(counts = 0)) %>%
  mutate(stage = factor(stage, levels = c('input', 'filtered', 'denoised', 'merged', 'tabled', 'nonchim'))) %>%
  ggplot(mapping = aes(x = stage, y = counts, by = sample, group = sample)) + geom_line(alpha = 0.5) + theme_classic()
```

## Assign Taxonomy
```{r}
taxa.redo <- assignTaxonomy(seqtab.nochim.redo, silva.ref, multithread = TRUE)
taxa.print.redo <- taxa.redo
rownames(taxa.print.redo) <- NULL
head(taxa.print.redo)
```

## Make Phyloseq Object
Load metadata
```{r}
metadata.df.redo = read_tsv(redo.map.file) %>%
  column_to_rownames("SampleID") %>%
  as.data.frame()

metadata.df.redo
```

Construct phyloseq object
```{r}
otus.redo = otu_table(seqtab.nochim.redo, taxa_are_rows = FALSE)
sd.redo = sample_data(metadata.df.redo)
redo.ps <- phyloseq(otus.redo, sd.redo, tax_table(taxa.redo))

redo.ps
```

Save phyloseq object as RDS
```{r}
write_rds(redo.ps, redo.ps.rds)
```

Confirm that the RDS is usable
```{r}
loaded.redo.ps = read_rds(redo.ps.rds)
print(loaded.redo.ps)
```

# Merge Phyloseq Objects

Remove bad samples from initial phyloseq object
```{r}
good.initial.ps <- loaded.ps %>%
  subset_samples(!SampleName %in% 
                   c("H3", "H4", "H15", "H16", "H25", "H27",
                     "1","12", "13","2086-Day7","PBS-Day14", 
                     "C1M1_ileum_lumen", "C1M1_ileum_tissue", "C2M2_ileum_lumen", "C2M2_ileum_tissue",
                     "C5M1_ileum_lumen", "C5M2_ileum_tissue", "C6M1_colon_lumen", "C6M1_ileum_lumen",
                     "C6M1_ileum_tissue"))
```

Remove bad samples from redone phyloseq object
```{r}
good.redone.ps <- loaded.redo.ps %>%
  subset_samples(!SampleName %in%
                   c("H16", "H25", "2086-Day7"))
```


Merge phyloseq objects
```{r}
#Merge phyloseq objects
merged.ps <- merge_phyloseq(good.initial.ps, good.redone.ps)
merged.ps
```


```{r}
#Save as new RDS
write_rds(merged.ps, merged.ps.rds)
```

Confirm that the merged RDS is usable
```{r}
loaded.merged.ps = read_rds(merged.ps.rds)
print(loaded.merged.ps)
```


# Session Info
```{r}
sessionInfo()
```

